{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./image.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "result = np.zeros((image.shape[0], 256), dtype=np.uint8)\n",
    "\n",
    "hist = cv2.calcHist([image],[0],None,[256],[0,256])\n",
    "cv2.normalize(hist, hist, 0, 256, cv2.NORM_MINMAX)\n",
    "\n",
    "for x, y in enumerate(hist):\n",
    "    cv2.line(result, (x, image.shape[0]), (x, image.shape[0]-y), 255)\n",
    "    \n",
    "dst = np.hstack([image[:,:,0],result])\n",
    "cv2.imshow(\"dst\",dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 5 6]\n",
      " [4 5 6]]\n",
      "[[[1.+0.j 2.+0.j 3.+0.j]]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.array([[1,2,3], [4,5,6]])\n",
    "array2 = np.array([1,2,3], dtype=complex, ndmin=3)\n",
    "\n",
    "array4 = np.array(np.mat('1 2; 3 4'), subok=True)\n",
    "\n",
    "array1[0] = [4,5,6]\n",
    "\n",
    "\n",
    "print(array1)\n",
    "print(array2)\n",
    "print(array4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  8  9 10]\n",
      " [11 12 13 14 15]]\n",
      "[[ 1  2  3  4  5]\n",
      " [11 12 13 14 15]]\n",
      "[12 14]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,2,3,4,5],\n",
    "                 [6,7,8,9,10],\n",
    "                 [11,12,13,14,15],\n",
    "                 [16,17,18,19,20]])\n",
    "\n",
    "print(array[1:3])\n",
    "print(array[::2])\n",
    "print(array[2, 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n",
      "[[ 0  2  4  6  8 10]\n",
      " [ 1  3  5  7  9 11]]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(12)\n",
    "\n",
    "reshape1 = array.reshape(2,3,2)\n",
    "reshape2 = np.reshape(array, (2,-1), order='F')\n",
    "\n",
    "print(reshape1)\n",
    "print(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[[0 1 2 3]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(4)\n",
    "\n",
    "axis1 = array[np.newaxis]\n",
    "axis2 = array[:, np.newaxis]\n",
    "\n",
    "\n",
    "flat1 = axis2.flatten(order='F')\n",
    "print(array)\n",
    "print(axis1)\n",
    "print(axis2)\n",
    "print(flat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9 10 11]]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.arange(6).reshape(2,3)\n",
    "array2 = np.arange(6, 12).reshape(2,3)\n",
    "\n",
    "merge1 = np.stack([array1, array2], axis=0)\n",
    "merge2 = np.stack([array1, array2], axis=-1)\n",
    "\n",
    "print(merge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  6]\n",
      "  [ 1  7]\n",
      "  [ 2  8]]\n",
      "\n",
      " [[ 3  9]\n",
      "  [ 4 10]\n",
      "  [ 5 11]]]\n"
     ]
    }
   ],
   "source": [
    "print(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n",
      "[array([[0, 1, 2, 3, 4]]), array([[5, 6, 7, 8, 9]])]\n",
      "[array([[0, 1],\n",
      "       [5, 6]]), array([[2],\n",
      "       [7]]), array([[3, 4],\n",
      "       [8, 9]])]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(10).reshape(2,5)\n",
    "\n",
    "detach1 = np.split(array, 2, axis=0)\n",
    "detach2 = np.split(array, [2,3], axis=1)\n",
    "\n",
    "print(array)\n",
    "print(detach1)\n",
    "print(detach2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[2.5 4.5]\n",
      " [4.5 6.5]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.array([1,2,3,4]).reshape(2,2)\n",
    "array2 = np.array([1.5, 2.5])\n",
    "print(array1)\n",
    "\n",
    "add = array1 + array2\n",
    "print(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1920, 3)\n",
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "array = np.zeros((1280, 1920, 3), np.uint8)\n",
    "x,y,w,h = 100, 100, 300, 300\n",
    "roi = array[x:x+w, y:y+h]\n",
    "\n",
    "print(array.shape)\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이미지 입력\n",
    "\n",
    "#### cv2.imread(이미지 경로, 플래그:이미지 입력 함수의 플래그)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (863, 1300) uint8\n",
      "[[118  91 136 ... 237 237 237]\n",
      " [109 102 109 ... 238 238 238]\n",
      " [129 147 121 ... 238 238 238]\n",
      " ...\n",
      " [160 145 135 ...  51  47  40]\n",
      " [ 99 141 146 ...  46  44  37]\n",
      " [ 45 132 163 ...  44  45  42]]\n"
     ]
    }
   ],
   "source": [
    "src = cv2.imread(\"./image.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(src.ndim, src.shape, src.dtype)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 이미지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"src\", flags = cv2.WINDOW_FREERATIO)\n",
    "cv2.resizeWindow(\"src\", 400, 200)\n",
    "cv2.imshow(\"src\",src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "cv2.namedWindow(winname, flags=None): 화면에 이미지를 표시할 수 있는 윈도우를 생성.</p>\n",
    "cv2.resizeWindow(winname, width, height): winname과 동일한 윈도우의 크기를 설정.</p>\n",
    "cv2.imshow(winname, ndarray): 윈도우에 이미지를 표시.</p>\n",
    "cv2.waitKey(int delay): 지정된 시간 동안 키 입력이 있을 때까지 프로그램을 지연시킴.</p>\n",
    "cv2.destroyWindow(string winName): 윈도우를 제거.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 동영상 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2. VideoCapture(\"./count.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if(capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        capture.open('./count.mp4')\n",
    "        \n",
    "    cv2.imshow(\"VideoFrame\", frame)\n",
    "    if cv2.waitKey(33) == ord('q'): break  # 키보드에서 q를 누르면 동영상 출력이 종료됨.\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while문: 동영상에서 프레임을 모두 표시하기 위해 사용. </p>\n",
    "capture.read(): 동영상 파일에서 프레임을 가져와서 압축을 해제한 다음 bool과 ndarray 타입의 값을 반환.</p>\n",
    "- ret(bool)은 capture 변수에서 정상적으로 프레임을 읽었는지를 나타내고, frame(ndarray)은 현재 프레임을 나타낸다. </p>\n",
    "\n",
    "현재 프레임수(cv2.CAP_PROP_POS_FRAMES)가 동영상의 총 프레임 수(cv2.CAP_PROP_FRAME_COUNT)와 동일할 때 동영상 파일을 다시 읽어서 capture 변수에 동영상을 할당 or break문을 통해 반복문 종료. </p>\n",
    "\n",
    "cv2.imshow(): 이미지로 저장된 프레임을 윈도우에 표시.</p>\n",
    "- 이를 위해 cv2.waitKey()를 사용하여 33ms만큼 대기한 후, 다음 프레임으로 넘어가게 함.\n",
    "- Python OpenCV는 문자를 처리하지 못하므로, 유니코드 값으로 변환하기 위해 ord()를 활용. </p>\n",
    "\n",
    "capture.release(): 동영상 재생이 끝나면 동영상 파일을 닫고 메모리르 해제하기 위해 호출.</p>\n",
    "cv2.destroyAllWindows(): 모든 윈도우를 제거 (동영상 파일의 재생이 끝났기 때문에)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 카메라 출력\n",
    "카메라가 스트리밍 형태로 동작할 때 사용. 즉, 데이터를 실시간으로 받아오고 분석해야 하는 경우 카메라를 이용해 데이터를 처리, </p>\n",
    "- Python OpenCVSharp의 카메라 출력 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captrue = cv2.VideoCapture(index)   #index: 카메라의 장치 번호."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹캡이 내장된 노트북이나 내장돼 있지 않은 컴퓨터에 카메라를 연결할 경우 장치 번호는 0. </p>\n",
    "카메라가 여러 대 연결돼 있다면 0이 아닌 1, 2, 3, ... 등의 장치 번호를 사용하여 외부 카메라 사용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow(\"VideoFrame\", frane)\n",
    "        if cv2.waitKey(33) == ord('q'): break\n",
    "            \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllwindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 도형 그리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영상이나 이미지 위에 그래픽을 그리는 것을 의미. OpenCv의 도형 그리기 함수는 주로 검출 결과를 시각적으로 표시하는 데 사용. 또한 이미지 위에 검출 결과를 새롭게 그려서 결과값을 변결하거나 보정하기 위해서도 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 타입: 도형을 그릴 때 어떤 유형의 선으로 그릴지 결정하는 인수. (브레젠험 알고리즘 방식(4-연결, 8-연결), 안티 엘리어싱 방식, 내부 채우기 방식이 존재)\n",
    "- 비트 시프트: 도형 그리기 함수에서 사용되는 일반적인 정수값 대신 비트 시프트를 활용하여 소수점 이하의 값이 포함된 실숫값 좌표로 도형 그리기 함수를 사용.\n",
    "- 직선 그리기: 이미지나 영상 위에 단순한 선을 그림. 주로 두 점을 이어서 검출된 결과를 사용자가 인식하기 쉽게 표시하거나 이미지의 특정 영역을 보정하기 위해 사용.\n",
    "- 사각형 그리기: 이미지나 영상 위에 단순한 사각형을 그림. 주로 관심 영역을 설정하기 위한 변숫값으로 활용하거나 검출된 결과를 사용자가 인식하기 쉽게 표시하는데 사용.\n",
    "- 원 그리기: 이미지나 영상 위에 단순한 원을 그림. 주로 검출된 좌표값을 사용자가 인식하기 쉽게 표시하는 데 사용함.\n",
    "- 호 그리기: 이미지나 영상 위에 단순한 호나 타원을 그림. 주로 검출된 타원을 그리거나 호를 그리거나 타원 객체의 부정확한 영역을 보정하기 위해 사용.\n",
    "- 내부가 채워지지 않은 다각형 그리기: 이미지나 영상 위에 여러 개의 다각형 곡선을 그림. 주로 복잡한 형상의 다각형을 그리거나 검출된 윤곽선의 일부를 시각적으로 확인할 때 사용됨.\n",
    "- 내부가 채워진 다각형 그리기: 이미지나 영상 위에 내부가 채워진 여러 개의 다각형 곡선을 그림. 주로 복잡한 형상의 다각형을 그리거나 결과를 이미지 위에 덮어 씌울 때 사용.\n",
    "- 문자 그리기: 이미지나 영상 위에 문자를 표시, 주로 검출된 결과에 시각적으로 라벨을 표시할 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((768, 1366, 3), dtype=np.uint8)\n",
    "\n",
    "cv2.line(img, (100,100),(1200, 100), (0,0,255), 3, cv2.LINE_AA)\n",
    "cv2.circle(img, (300, 300), 50, (0,255,0), cv2.FILLED, cv2.LINE_4)\n",
    "cv2.rectangle(img, (500, 200), (1000, 400), (255,0,0), 5, cv2.LINE_8)\n",
    "cv2.ellipse(img, (1200, 300), (100, 50), 0, 90, 180, (255,255,0),2)\n",
    "\n",
    "pts1 = np.array([[[100,500], [300,500], [200,600]], [[400,500], [500,500], [600,700]]])\n",
    "pts2 = np.array([[700,500], [800,500], [700,600]])\n",
    "cv2.polylines(img, pts1, True, (0, 255,255),2)\n",
    "cv2.fillPoly(img, [pts2], (255,0,255), cv2.LINE_AA)\n",
    "\n",
    "cv2.putText(img, 'OpneCV', (900,600), cv2.FONT_HERSHEY_COMPLEX | cv2.FONT_ITALIC, 2, (255,255,255), 3)\n",
    "\n",
    "cv2.imshow('lmg', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 결과 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bd3902f3a946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./CV.jpeg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMWRITE_JPEG_QUALITY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMWRITE_JPEG_PROGRESSIVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "save = cv2.imwrite('./CV.jpeg', img, (cv2.IMWRITE_JPEG_QUALITY, 100, cv2.IMWRITE_JPEG_PROGRESSIVE, 1))\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 동영상 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV에서는 동영상을 저장할 때 프레임의 변경이나 변형을 녹화해서 저장할 수 있다. 동영상 저장 함수는 이미지 저장 함수와 동일하게 파일명의 확장자와 설정된 코덱을 읽어 기록한다. 또한, 새롭게 파일을 생성함으로 함수를 호출할 때 사용할 코덱, 프레임 속도, 프레임 등을 입력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('./count.mp4')\n",
    "width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "# width, height를 선언해서 녹화할 동영상의 프레임 크기를 받아온다.\n",
    "# 프레임 크기는 float 형식으로 반환되므로, int 형식으로 변경함. (변경하지 않을 경우, 동영상 저장 함수에서 오류가 발생함.)\n",
    "\n",
    "videoWriter = cv2.VideoWriter()\n",
    "# cv2.VideoWriter()를 통해 녹화를 위한 메모리 할당.\n",
    "\n",
    "isWrite = False\n",
    "# isWrite는 특정 프레임만 녹화하기 위해 구분하는 bool 형식 변수.\n",
    "\n",
    "\n",
    "while True:\n",
    "# isWrite가 True일 때만 녹화를 시작함.\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if(capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        capture.open('./count.mp4')\n",
    "        \n",
    "    cv2.imshow('VideoFrame', frame)\n",
    "    key = cv2.waitKey(33)\n",
    "    \n",
    "    \n",
    "    if key == 4:  # Alt + D 의미. 해당 키가 입력 되어있을때, 녹화를 시작함.\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID') # cv2.VideoWriter_fourcc(*'XVID')를 활용하여 코덱을 설정.\n",
    "        \n",
    "        videoWriter.open('./Video.avi', fourcc, 30, (width, height), True)\n",
    "        isWrite = True\n",
    "        \n",
    "    elif key == 24:  # Alt + X 의미, 해당 키가 입력되었을때, 녹화를 종료함.\n",
    "        videoWriter.release()  # 메모리 할당을 해제하고 녹화 함수를 종료함.\n",
    "        isWrite = False\n",
    "        \n",
    "    elif key == ord('q'): break\n",
    "        \n",
    "    if isWrite == True:\n",
    "        videoWriter.write(frame)\n",
    "        \n",
    "videoWriter.release()\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 변형\n",
    "## 1. 색상 공간 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본래의 색상 공간에서 다른 생삭 공간으로 변환활 때 사용. 색상 공간 변환 함수는 데이터 타입을 같게 유지하고 채널을 변환함. </p>\n",
    "색상 공간 변환 함수: 입력 이미지(src)에 색상 변환 코드를 적용해서 출력 이미지(dst)로 변환. (색상 변환 코드를 사용해서 BGR 색상 공간을 RGBA 색상 공간으로 변환하거나 그레이스케일, HSV, CIE, Luv 등 단일 채널부터 3채널, 4채널의 색상 공간으로도 변환이 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "dst = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)  # 다중 채널 색상 이미지(HSV)로 변환\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HSV 색상 공간\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "색상을 표현하기에 가장 간편한 색상 공간. 이미지에서 색상을 검출한다고 가정할 때, BGR이나, RGB 패턴으로는 인간이 인지하는 영역의 색상을 구별하기에는 매우 어렵고 복잡하지만, HSV 색상 공간을 활용하면 간편하고 빠르게 특정 색상을 검출하고 분리할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "색상을 검출하려면 각 매개변수에 색상의 범위를 상수 값으로 할당해야함. 그러므로 각 속성이 어떤 역할을 하는지, 어떤 범위를 갖는지 등을 충분히 이해해야함. </p>\n",
    "- 색상(Hue): 세 속성 중 하나로, 색깔의 질을 의미. 빨간, 노랑, 파랑 등의 표현으로 나타내는 성질.\n",
    "- 채도(Saturation): 색의 선명도를 의미하며, 아무것도 섞지 않아 맑고 깨끗하며 원색에 가까운 것을 채도가 높다고 표현.\n",
    "- 명도(Value): 색의 밝기. 명도가 높을수록 백색에, 낮을수록 흑색에 가까워짐.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이미지에서 빨간색 색상을 검출한다면 원형 색상의 범위로 인해 최솟값과 최댓값의 범위를 지정하는 데 어려움을 겪는다. 단순히 최솟값을 약 170, 최댓값을 약 10으로 설정해서 검출을 진행할 경우 오류가 발생한다. 이를 해결하기 위해 두 번에 걸쳐 색상 채널을 나누고 합치는 연산을 해야한다.</p>\n",
    "- 빨간색 계열을 나눠서 낮은 쪽의 빨간색 채널, 높은 쪽의 빨간색 채널을 만든 후 채녈을 합산함. 이를 위해 먼저 색상 공간을 채널별로 나눠야함.\n",
    "\n",
    "\n",
    "### 채널 분리 함수: cv2.split()\n",
    "\n",
    "채널 분리 함수는 다중 입력 이미지(src)를 단일 채널 이미지 배열(mv)로 나눈다. 3채널 이미지를 분리할 경우 단일 채널 이미지로 나눠져 세 개의 결과 이미지로 생성된다. mv 배열 안에는 첫 번째 채널 (mv[0]), 두 번째 채널(mv[1]), 세 번째 채널(mv[2])이 포함돼 있다. Python OpenCV에서는 리스트 형태로 반환됨.</p>\n",
    "\n",
    "### 채널 병합 함수: cv2.merge()\n",
    "\n",
    "채널 병합 함수는 단일 채널 이미지 배열(mv)를 병합해 하나의 출력 이미지(dst)로 반환한다. 채널 분리 함수와 반대로 작동하며, mv 배열 안에는 첫 번째 채널(mv[0]), 두 번째 채널(mv[1]), 세 번째 채널(mv[2])이 포함돼야 한다. 또한 mv 배열의 첫 번째 채널이 채널 별하의 기준이 되어 모든 채널의 속성이 첫 번째 채널의 속성과 일치해야 한다.\n",
    "\n",
    "\n",
    "### 배열 요소의 범위 설정 함수: cv2.InRange()\n",
    "\n",
    "다중 채널 이미지에서 단일 채널을 갖는 이미지들로 분리했다면 해당 채널에서 특정 범위의 값으로 검출해야함. 검출하려는 값과 일치하는 범위는 255를 할당하고 검출하려는 값과 일치하지 않는 범위는 0의 값을 할당한다. 이때 배열 요소의 범위 설정 함수를 사용.</p>\n",
    "\n",
    "배열 요소 범위 설정 함수는 입력 이미지(src)에서 낮은 범위(lowerb)에서 높은 범위(upperb) 사이의 요소를 검출한다. 범위 안에 포함되는 값을 255로 변경하며, 포함되지 않는 값은 0으로 변경해서 출력 이미지(dst)로 반환한다.</p>\n",
    "\n",
    "Python OpenCV에서는 튜플 자료형을 사용해 범위를 할당함. 단일 채널 이미지의 경우 int 형식으로 v값을 할당해서 사용하며, 다중 채널 이미지의 경우 (v0, v1, v2)형식으로 할당한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)  #원본 이미지를 HSV 색상 공간 이미지로 변환.\n",
    "\n",
    "h, s, v = cv2.split(hsv)  # HSV 색상 공간(배열 이미지)에 채널을 분리하여 할당.\n",
    "h_red = cv2.inRange(h,0,5)  # Hue채널에 범위를 할당한 후 빨간색 색상 범위를 갖는 객체만 검출.\n",
    "\n",
    "dst = cv2.bitwise_and(hsv, hsv, mask=h_red)  \n",
    "# h_red 이미지는 Hue 채널에서 0~5 사이의 값을 지니는 요서만 255 값으로 변경하고 나머지는 모두 0으로 변경한다.\n",
    "# 특정 요소가 검출이 완료되면 HSV 이미지 위에 마스크를 씌워 검출된 요소만 보이게 한다.\n",
    "\n",
    "dst = cv2.cvtColor(dst, cv2.COLOR_HSV2BGR)\n",
    "# cv2.imshow()함수는 BGR색상 공간만 정상적으로 출력하므로, HSV 색상 공간을 다시 BGR 색상 공간으로 변경한다.\n",
    "\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비교적 우수하게 검풀에 성공했지만 부정확한 요소를 검출하거나 거의 완벽하게 검출에 성공하지는 못했다. 단순히 HSV 공간 중 Hue의 공간만 활용해 검출했지 때문이다. 도한 붉은색은 앞선 Hue 공간의 범위에서 알 수 잇듯이, 170 이상의 값도 붉은 색에 포함된다. </p>\n",
    "이 문제를 해결하려면 배열 요소의 범위 설정 함수를 HSV 색상 공간으로 범위를 설정하고 검출한 두 요소의 배열을 병합해서 하나의 공간으로 만들어야 한다.</p>\n",
    "\n",
    "\n",
    "### 배열 병합 함수: cv2.addWeighted()\n",
    "\n",
    "배열 병합 함수는 입력 이미지1(src1)에 대한 가중치(alpha) 곱과 입력 이미지2(src2)에 대한 가중치(beta) 곱의 합 중 추가 합(gamma)를 더해서 계산한다. 선택 깊이(dtype)는 정밀도를 임의로 설정할 수 있다. 기본값을 사용할 경우 입력 이미지1(src1)의 정밀도로 설정된다.</p>\n",
    "\n",
    "- 배열 병합 함수 수식: dst = (src1 * alpha) + (src2 * beta) + gamma\n",
    "\n",
    "배열 병합 함수는 알파 블렌딩(alpha blending)을 구현할 수 있어서 서로 다른 이미지를 불투명하게 혼합해서 표시할 수 있다.</p>\n",
    "입력 이미지1과 입력 이미지2를 어떠한 변화 없이 사용할 경우, alpha값은 1.0, beta값은 0.0으로 할당해서 사용한다. 출력 이미지는 두 입력 이미지의 정밀도가 같으므로 기본값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv2.split(hsv)\n",
    "\n",
    "orange = cv2.inRange(hsv, (8, 100, 100), (20, 255, 255))\n",
    "blue = cv2.inRange(hsv, (110, 100, 100), (130, 255, 255))\n",
    "mix_color = cv2.addWeighted(orange, 10., blue, 1.0, 0,0)   \n",
    "# 배열 병합 함수를 활용해 orangr와 blue의 배열을 병합해 mix_color의 배열로 출력한다.\n",
    "# 함수에서 가중치의 할당 없이 병합하므로, alpha는 1.0, beta는 0.0의 값을 사용한다.\n",
    "\n",
    "dst = cv2.bitwise_and(hsv, hsv, mask =mix_color)\n",
    "dst = cv2.cvtColor(dst, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 출력 결과에서 주황색 객체와 파란색 객체 두 가지 색상이 반환된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 이진화\n",
    "동영상이나  이미지에서 어느 지점을 기준으로 픽셀을 분류해서 제외해야 할 때가 있다. 이때 특정 값을 기준으로 값이 높거나 낮은 픽셀을 검은색 또는 흰색의 값으로 변경한다. 즉, 기준값에 따라 이분법적으로 구분해서 픽셀을 참 또는 거짓으로 나누는 연산이며, 이미지 행렬에서 모든 픽셀에 대해 이러한 연산을 수행하는 것이 이진화. </p>\n",
    "\n",
    "\n",
    "### 이진화 함수: cv2.threshold()\n",
    "이진화 함수는 입력 이미지(src)를 임곗값 형식(type)에 따라 특정한 비교 연산을 진행한다. 임곗값(thresh)보다 낮은 픽셀값은 0이나 원본 필셀값으로 변경하며, 임곗값(thresh)보다 높은 픽셀값은 최댓값(maxval)으로 변경한다. 변형된 이미지는 출력 이미지(dst)에 저장되며, Python OpenCV에서는 설정 임곗값(retval)도 반환된다.</p>\n",
    "\n",
    "일반적으로 이진화 함수는 단일 채널 이미지에서 활용되며, 다중 채널 이미지에 이진화 함수를 적용할 경우 각 채널을 분리해서 이진화 함수를 적용한 후 이미지를 다시 병합해서 반환한다. 특정 임곗값 형식에서는 단일 채널 이미지만을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "_, binary = cv2.threshold(src, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('binary', binary)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 이미지에 직접적으로 이진화 함수를 적용한 예제. 색상이 극단적으로 표현되는 것을 알 수 있다. 채널마다 임곗값을 적용해서 반환하므로,  각 채널은 두 종류의 값으로 나뉜다. 결국 다중 채널 이미지에 이진화가 적용된 채널들이 다시 병합되어 하나의 이미지로 변해서 활용하기 어려운 이미지가 된다. 이러한 문제로 특별한 경우가 아닌 이상 다중 채널 이미지에는 이진화 함수를 적용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오츠 알고리즘\n",
    "입력된 이미지의 밝기 분포(히스토그램)를 통해 최적의 임곗값을 찾아 이진화를 적용하는 알고리즘. 가능한 모든 임곗값을 고려해서 이미지 내의 픽셀들을 두 개의 클래스로 분류했을 때 클래스 간의 분산을 최소화하거나 차이를 최대화하는 임곗값을 찾는다.</p>\n",
    "- 오츠 알고리즘 수식: ![image.png](./수식1.png)\n",
    "\n",
    "(수식을 최소화하는 임곗값을 찾는다.)\n",
    "\n",
    "\n",
    "### 삼각형 알고리즘\n",
    "오츠 알고리즘과 동일하게 입력된 이미지의 밝기 분포(히스토그램)를 통해 최적의 임곗값을 찾아 이진화를 적용하는 알고리즘. 오츠 알고리즘과는 다르게 모든 임곗값을 대입하지는 않음. 삼각형 알고리즘은 히스토그램에서 최대 거리를 구성할 수 있는 임곗값을 찾아 이진화를 적용. </p>\n",
    "이때, 최대 거리를 구성할 수 있는 임곗값을 찾아 직각 삼각형으로 만드는 것. 삼각형의 빗변 사이의 거리가 최대일 때 수직인 선이 히스토그램의 최대 거리가 된다. 즉, 히스토그램에 그려진 선 사이의 거리가 최대인 지역값이 임곗값이 됨.</p>\n",
    "\n",
    "\n",
    "#### 적응형 이진화 알고리즘\n",
    "입력 이미지에 따라 입곗값이 스스로 다른 값을 할당할 수 있도록 구성된 이진화 알고리즘. 이미지에 따라 어떠한 임곗값을 주더라도 이진화 처리가 어려운 이미지가 존재함. 이러한 경우 적응형 이진화 알고리즘을 적용.\n",
    "\n",
    "- 적응형 이진화 함수: cv2.adaptiveThreshold()\n",
    "\n",
    "이진화 함수에서 사용하는 최댓값 플래그와 임곗값 형식 플래그를 동일하게 사용. 각 픽셀 주변의 blockSize X blockSize 영역에 대한 가중 평균을 계산. 이후 가중 평균에서 상수 C를 감산한 값을 계산해서 픽셀마다 적응형 임곗값 T(x,y)를 설정. </p>\n",
    "주변 영역 크기인 blockSize와 상수 C에 따라 설정되는 임곗값의 결과가 크게 달라진다. blockSize는 중심점이 존재할 수 있게 홀수만 가능하며 상수 C는 일반적으로 양수의 값을 사용하지만 경우에 따라 0이나 음수도 사용 가능. 또한 적응형 이진화 방식에 따라 결과가 변화며 OpenCV는 두 가지 알고리즘을 지원함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 33, -5)\n",
    "\n",
    "cv2.imshow('binary', binary)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이스케일 이미지에 평균 가중치를 적용한 적응형 이진화. blockSize에 33을 지정하여 33X33 크기 내의 영역을 분석해 적절한 임곗값을 설정한다. 상수 C에는 음수 값인 -5를 지정해 전체 영역이 어두워짐. 음수 값을 지정할 때는 임곗값 형식(thresholdType)에 반전 이진화 플래그(cv.THRESH_BINARY_INV)를 적용하거나 이미지 반전 역산을 적용해 현재 구상하는 알고리즘에 맞는 데이터를 얻어낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지 연산\n",
    "하나 또는 둘 이상의 이미지에 대해 수학적인 연산을 수행하는 것.\n",
    "\n",
    "\n",
    "### 덧셈 함수\n",
    "- cv2.add()\n",
    "\n",
    "배열과 배열 또는 배열과 스칼라의 요소별 합을 계산함. 정밀도에 따라 요소의 최댓값과 최솟값이 있으며, 최댓값을 넘어가거나 최솟값보다 낮아질 수 없다. 덧셈 함수는 최댓값을 넘어가기 쉽기 때문에 이미지 연산 시 두 배열의 요솟값을 고려해서 사용한다.\n",
    "\n",
    "### 뺄셈 함수\n",
    "- cv2.subtract()\n",
    "\n",
    "### 곱셈 함수\n",
    "- cv2.multiply()\n",
    "\n",
    "### 나눗셈 함수\n",
    "- cv2.divide()\n",
    "\n",
    "### 최댓값 함수\n",
    "- cv2.max()\n",
    "\n",
    "\n",
    "### 최솟값 함수\n",
    "- cv2.min()\n",
    "\n",
    "### 절댓값 함수\n",
    "- cv2.abs()\n",
    "\n",
    "### 절댓값 차이 함수\n",
    "- cv2.absdiff()\n",
    "\n",
    "### 비교 함수\n",
    "- cv2.compare()\n",
    "\n",
    "### 선형 방정식 시스템의 해 찾기 함수\n",
    "- cv2.solve()\n",
    "\n",
    "역함수를 기반으로 선형 시스템의 해를 빠르게 구해서 반환함. 부동 소수점 형식만 지원됨.\n",
    "\n",
    "### AND 연산 함수\n",
    "- cv2.bitwise_and()\n",
    "\n",
    "### OR 연산 함수\n",
    "- cv2.bitwise_or()\n",
    "\n",
    "### XOR 연산 함수\n",
    "- cv2.bitwise_xor()\n",
    "\n",
    "### NOT 연산 함수\n",
    "- cv2.bitwise_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, array([[4.],\n",
      "       [1.]]))\n"
     ]
    }
   ],
   "source": [
    "src1 = np.array([[9,2], [1,1]], dtype=np.double)\n",
    "src2 = np.array([38,5], dtype=np.double)\n",
    "\n",
    "dst = cv2.solve(src1, src2, flags=cv2.DECOMP_LU)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 흐림효과\n",
    "블러링(Blurring) 또는 스무딩(smoothing)이라 불리며, 노이즈를 줄이거나 외부 영향을 최소화하는 데 사용된다. 흐림 효과는 단순히 이미지를 흐리게 만드는 것뿐만 아니라 노이즈를 제거해서 연산 시 계산을 빠르고 정확하게 수행하는 데 도움이 된다. 또한 이미지의 해상도를 변경하는 경우에도 사용되는데 이미지의 크기를 변경하면 존재하지 않는 데이터를 생성하거나 존재하는 데이터를 줄여야 하므로 샘플링된 이미지를 재구성할 때 사용된다.</p>\n",
    "\n",
    "흐림 효과는 영상이나 이미지를 번지게 하며, 해당 픽셀의 주변 값들과 비교하고 게산해서 픽셀들의 색상을 재조정한다. 흐림 효과 함수는 크게 다섯가지의 종류가 있으며, 세 가지 중요 매개변수가 있다. 이 매개변수에 따라 이미지에 흐림 효과를 어떻게 처리할 지가 결정된다.</p>\n",
    "\n",
    "\n",
    "#### 커널과 고정점\n",
    "커널(Kernel)은 이미지에서 (x,y)의 픽셀과 해당 픽셀 주변을 포함한 작은 크기의 공간을 의미하며, 이 영역 각각에 특정한 수식이나 함수 등을 적용해 새로운 이미지를 얻는 알고리즘에서 사용된다. 커널은 영역의 형태와 요소가 결합되는 방식을 정의하는 템플릿을 의미하기도 한다.</p>\n",
    "\n",
    "새로운 픽셀을 만들어 내기 위해 커널 크기의 화소 값을 이용해 어떤 시스템을 통괗 계산하는 것을 컨볼루션(Convolution)이라 한다. 컨벌루션의 예로 이미지를 흐리게 만드는 블러링(Blurring), 이미지의 윤곽을 선명하게 만드는 샤프닝(Sharpening), 이미지 명도의 변화량을 구하는 미분(Gradient, Laplaian) 등이 있다.</p>\n",
    "\n",
    "고정점(anchor point)은 커널을 통해 컨볼루션된 값을 할당한 지점이다. 커널 내에서 고정점은 하나의 지점을 가지며, 이미지와 어떻게 정렬되는지를 나타낸다. </p>\n",
    "\n",
    "\n",
    "#### 테두리 외삽법\n",
    "컨볼루션을 적용할 때 이미지의 가장자리 부분은 계산이 불가능하다. 커널을 활용하는 연산은 모두 이러한 문제에 부딪힌다. 이 문제를 해결하기 위해 텥두리의 이미지 바깥족에 가상의 픽셀을 만들어 처리한다. </p>\n",
    "외삽법으로 가상의 픽셀의 값을 할당하는데, 가상 픽셀의 값을 0으로 처리하거나 커널이 연산할 수 잇는 부분부터 연산을 수행하기도 한다. 또는 이미지의 시작과 끝을 연결해서 폐곡선을 형성해 이미지의 테두리 부분을 대신하게 한다. 커널을 활용하는 함수는 대부분 테두리 외삽법을 설정하는 매개변수를 가지고 있다.</p>\n",
    "\n",
    "\n",
    "### 단순 흐림 효과\n",
    "- cv.blur(src, ksize, anchor=None, borderType=None)\n",
    "\n",
    "입력 이미지(src)의 각 픽셀에 대해 커널을 적용해 모든 픽셀의 단순 평균을 구해 출력 이미지(dst)에 저장한다. 커널의 크기는 ksize를 통해 설정하며, anchor는 커널을 정렬하는 방식을 지정해 고정점을 설정하는 데 사용된다. null값이나 None값을 사용하면 고정점의 위치는 (-1,-1)을 갖게된다. 이 값은 커널을 기준으로 중앙에 위치함을 의미한다. 3X3 크기의 커널일 경우 중심점은 (1,1)이므로 (-1,-1)은 (1,1)의 위치를 갖는다. 다중 채널 이미지의 경우 채널별로 단순 평균값이 계산된다. borderType은 테두리 외삽법을 의미한다.</p>\n",
    "\n",
    "\n",
    "### 박스 필터 흐림 효과\n",
    "- cv2.boxFilter(src, ddepth, ksize, anchor=None, normalize=None, borderType=None)\n",
    "\n",
    "커널의 내부 값이 모두 값은 필터. ddepth는 출력 이미지의 정밀도를 설정한다. ddepth값을 cv2.CV_8U 형식으로 입력하며, -1로 설정한다면 입력 이미지와 동일한 정밀도가 설정된다. normalize는 커널이 영역별 정규화 여부를 설정ㅎㄴ다. 박스 필터 함수는 일반적으로 커널의 모든 값이 1의 값을 갖는다. 하지만 normalize의 값을 True로 지정할 경우 정규화된 박스 필터로 변경되며, 커널의 모든 값이 커널의 개수만큼 나눠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
