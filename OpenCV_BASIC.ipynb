{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./draw.JPG')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "result = np.zeros((image.shape[0], 256), dtype=np.uint8)\n",
    "\n",
    "hist = cv2.calcHist([image],[0],None,[256],[0,256])\n",
    "cv2.normalize(hist, hist, 0, 256, cv2.NORM_MINMAX)\n",
    "\n",
    "for x, y in enumerate(hist):\n",
    "    cv2.line(result, (x, image.shape[0]), (x, image.shape[0]-y), 255)\n",
    "    \n",
    "dst = np.hstack([image[:,:,0],result])\n",
    "cv2.imshow(\"dst\",dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 5 6]\n",
      " [4 5 6]]\n",
      "[[[1.+0.j 2.+0.j 3.+0.j]]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.array([[1,2,3], [4,5,6]])\n",
    "array2 = np.array([1,2,3], dtype=complex, ndmin=3)\n",
    "\n",
    "array4 = np.array(np.mat('1 2; 3 4'), subok=True)\n",
    "\n",
    "array1[0] = [4,5,6]\n",
    "\n",
    "\n",
    "print(array1)\n",
    "print(array2)\n",
    "print(array4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  7  8  9 10]\n",
      " [11 12 13 14 15]]\n",
      "[[ 1  2  3  4  5]\n",
      " [11 12 13 14 15]]\n",
      "[12 14]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,2,3,4,5],\n",
    "                 [6,7,8,9,10],\n",
    "                 [11,12,13,14,15],\n",
    "                 [16,17,18,19,20]])\n",
    "\n",
    "print(array[1:3])\n",
    "print(array[::2])\n",
    "print(array[2, 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n",
      "[[ 0  2  4  6  8 10]\n",
      " [ 1  3  5  7  9 11]]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(12)\n",
    "\n",
    "reshape1 = array.reshape(2,3,2)\n",
    "reshape2 = np.reshape(array, (2,-1), order='F')\n",
    "\n",
    "print(reshape1)\n",
    "print(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[[0 1 2 3]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(4)\n",
    "\n",
    "axis1 = array[np.newaxis]\n",
    "axis2 = array[:, np.newaxis]\n",
    "\n",
    "\n",
    "flat1 = axis2.flatten(order='F')\n",
    "print(array)\n",
    "print(axis1)\n",
    "print(axis2)\n",
    "print(flat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9 10 11]]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.arange(6).reshape(2,3)\n",
    "array2 = np.arange(6, 12).reshape(2,3)\n",
    "\n",
    "merge1 = np.stack([array1, array2], axis=0)\n",
    "merge2 = np.stack([array1, array2], axis=-1)\n",
    "\n",
    "print(merge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  6]\n",
      "  [ 1  7]\n",
      "  [ 2  8]]\n",
      "\n",
      " [[ 3  9]\n",
      "  [ 4 10]\n",
      "  [ 5 11]]]\n"
     ]
    }
   ],
   "source": [
    "print(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n",
      "[array([[0, 1, 2, 3, 4]]), array([[5, 6, 7, 8, 9]])]\n",
      "[array([[0, 1],\n",
      "       [5, 6]]), array([[2],\n",
      "       [7]]), array([[3, 4],\n",
      "       [8, 9]])]\n"
     ]
    }
   ],
   "source": [
    "array = np.arange(10).reshape(2,5)\n",
    "\n",
    "detach1 = np.split(array, 2, axis=0)\n",
    "detach2 = np.split(array, [2,3], axis=1)\n",
    "\n",
    "print(array)\n",
    "print(detach1)\n",
    "print(detach2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[2.5 4.5]\n",
      " [4.5 6.5]]\n"
     ]
    }
   ],
   "source": [
    "array1 = np.array([1,2,3,4]).reshape(2,2)\n",
    "array2 = np.array([1.5, 2.5])\n",
    "print(array1)\n",
    "\n",
    "add = array1 + array2\n",
    "print(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1920, 3)\n",
      "(300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "array = np.zeros((1280, 1920, 3), np.uint8)\n",
    "x,y,w,h = 100, 100, 300, 300\n",
    "roi = array[x:x+w, y:y+h]\n",
    "\n",
    "print(array.shape)\n",
    "print(roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이미지 입력\n",
    "\n",
    "#### cv2.imread(이미지 경로, 플래그:이미지 입력 함수의 플래그)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (967, 1525) uint8\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  3 71  0]\n",
      " [ 0  0  0 ... 78  7  5]\n",
      " [ 0  0  0 ... 72  0  0]]\n"
     ]
    }
   ],
   "source": [
    "src = cv2.imread(\"./draw.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(src.ndim, src.shape, src.dtype)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 이미지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"src\", flags = cv2.WINDOW_FREERATIO)\n",
    "cv2.resizeWindow(\"src\", 400, 200)\n",
    "cv2.imshow(\"src\",src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "cv2.namedWindow(winname, flags=None): 화면에 이미지를 표시할 수 있는 윈도우를 생성.</p>\n",
    "cv2.resizeWindow(winname, width, height): winname과 동일한 윈도우의 크기를 설정.</p>\n",
    "cv2.imshow(winname, ndarray): 윈도우에 이미지를 표시.</p>\n",
    "cv2.waitKey(int delay): 지정된 시간 동안 키 입력이 있을 때까지 프로그램을 지연시킴.</p>\n",
    "cv2.destroyWindow(string winName): 윈도우를 제거.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 동영상 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2. VideoCapture(\"./count.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if(capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        capture.open('./count.mp4')\n",
    "        \n",
    "    cv2.imshow(\"VideoFrame\", frame)\n",
    "    if cv2.waitKey(33) == ord('q'): break  # 키보드에서 q를 누르면 동영상 출력이 종료됨.\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while문: 동영상에서 프레임을 모두 표시하기 위해 사용. </p>\n",
    "capture.read(): 동영상 파일에서 프레임을 가져와서 압축을 해제한 다음 bool과 ndarray 타입의 값을 반환.</p>\n",
    "- ret(bool)은 capture 변수에서 정상적으로 프레임을 읽었는지를 나타내고, frame(ndarray)은 현재 프레임을 나타낸다. </p>\n",
    "\n",
    "현재 프레임수(cv2.CAP_PROP_POS_FRAMES)가 동영상의 총 프레임 수(cv2.CAP_PROP_FRAME_COUNT)와 동일할 때 동영상 파일을 다시 읽어서 capture 변수에 동영상을 할당 or break문을 통해 반복문 종료. </p>\n",
    "\n",
    "cv2.imshow(): 이미지로 저장된 프레임을 윈도우에 표시.</p>\n",
    "- 이를 위해 cv2.waitKey()를 사용하여 33ms만큼 대기한 후, 다음 프레임으로 넘어가게 함.\n",
    "- Python OpenCV는 문자를 처리하지 못하므로, 유니코드 값으로 변환하기 위해 ord()를 활용. </p>\n",
    "\n",
    "capture.release(): 동영상 재생이 끝나면 동영상 파일을 닫고 메모리르 해제하기 위해 호출.</p>\n",
    "cv2.destroyAllWindows(): 모든 윈도우를 제거 (동영상 파일의 재생이 끝났기 때문에)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 카메라 출력\n",
    "카메라가 스트리밍 형태로 동작할 때 사용. 즉, 데이터를 실시간으로 받아오고 분석해야 하는 경우 카메라를 이용해 데이터를 처리, </p>\n",
    "- Python OpenCVSharp의 카메라 출력 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captrue = cv2.VideoCapture(index)   #index: 카메라의 장치 번호."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹캡이 내장된 노트북이나 내장돼 있지 않은 컴퓨터에 카메라를 연결할 경우 장치 번호는 0. </p>\n",
    "카메라가 여러 대 연결돼 있다면 0이 아닌 1, 2, 3, ... 등의 장치 번호를 사용하여 외부 카메라 사용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow(\"VideoFrame\", frane)\n",
    "        if cv2.waitKey(33) == ord('q'): break\n",
    "            \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllwindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 도형 그리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영상이나 이미지 위에 그래픽을 그리는 것을 의미. OpenCv의 도형 그리기 함수는 주로 검출 결과를 시각적으로 표시하는 데 사용. 또한 이미지 위에 검출 결과를 새롭게 그려서 결과값을 변결하거나 보정하기 위해서도 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 타입: 도형을 그릴 때 어떤 유형의 선으로 그릴지 결정하는 인수. (브레젠험 알고리즘 방식(4-연결, 8-연결), 안티 엘리어싱 방식, 내부 채우기 방식이 존재)\n",
    "- 비트 시프트: 도형 그리기 함수에서 사용되는 일반적인 정수값 대신 비트 시프트를 활용하여 소수점 이하의 값이 포함된 실숫값 좌표로 도형 그리기 함수를 사용.\n",
    "- 직선 그리기: 이미지나 영상 위에 단순한 선을 그림. 주로 두 점을 이어서 검출된 결과를 사용자가 인식하기 쉽게 표시하거나 이미지의 특정 영역을 보정하기 위해 사용.\n",
    "- 사각형 그리기: 이미지나 영상 위에 단순한 사각형을 그림. 주로 관심 영역을 설정하기 위한 변숫값으로 활용하거나 검출된 결과를 사용자가 인식하기 쉽게 표시하는데 사용.\n",
    "- 원 그리기: 이미지나 영상 위에 단순한 원을 그림. 주로 검출된 좌표값을 사용자가 인식하기 쉽게 표시하는 데 사용함.\n",
    "- 호 그리기: 이미지나 영상 위에 단순한 호나 타원을 그림. 주로 검출된 타원을 그리거나 호를 그리거나 타원 객체의 부정확한 영역을 보정하기 위해 사용.\n",
    "- 내부가 채워지지 않은 다각형 그리기: 이미지나 영상 위에 여러 개의 다각형 곡선을 그림. 주로 복잡한 형상의 다각형을 그리거나 검출된 윤곽선의 일부를 시각적으로 확인할 때 사용됨.\n",
    "- 내부가 채워진 다각형 그리기: 이미지나 영상 위에 내부가 채워진 여러 개의 다각형 곡선을 그림. 주로 복잡한 형상의 다각형을 그리거나 결과를 이미지 위에 덮어 씌울 때 사용.\n",
    "- 문자 그리기: 이미지나 영상 위에 문자를 표시, 주로 검출된 결과에 시각적으로 라벨을 표시할 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((768, 1366, 3), dtype=np.uint8)\n",
    "\n",
    "cv2.line(img, (100,100),(1200, 100), (0,0,255), 3, cv2.LINE_AA)\n",
    "cv2.circle(img, (300, 300), 50, (0,255,0), cv2.FILLED, cv2.LINE_4)\n",
    "cv2.rectangle(img, (500, 200), (1000, 400), (255,0,0), 5, cv2.LINE_8)\n",
    "cv2.ellipse(img, (1200, 300), (100, 50), 0, 90, 180, (255,255,0),2)\n",
    "\n",
    "pts1 = np.array([[[100,500], [300,500], [200,600]], [[400,500], [500,500], [600,700]]])\n",
    "pts2 = np.array([[700,500], [800,500], [700,600]])\n",
    "cv2.polylines(img, pts1, True, (0, 255,255),2)\n",
    "cv2.fillPoly(img, [pts2], (255,0,255), cv2.LINE_AA)\n",
    "\n",
    "cv2.putText(img, 'OpneCV', (900,600), cv2.FONT_HERSHEY_COMPLEX | cv2.FONT_ITALIC, 2, (255,255,255), 3)\n",
    "\n",
    "cv2.imshow('lmg', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 결과 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bd3902f3a946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./CV.jpeg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMWRITE_JPEG_QUALITY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMWRITE_JPEG_PROGRESSIVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "save = cv2.imwrite('./CV.jpeg', img, (cv2.IMWRITE_JPEG_QUALITY, 100, cv2.IMWRITE_JPEG_PROGRESSIVE, 1))\n",
    "print(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 동영상 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV에서는 동영상을 저장할 때 프레임의 변경이나 변형을 녹화해서 저장할 수 있다. 동영상 저장 함수는 이미지 저장 함수와 동일하게 파일명의 확장자와 설정된 코덱을 읽어 기록한다. 또한, 새롭게 파일을 생성함으로 함수를 호출할 때 사용할 코덱, 프레임 속도, 프레임 등을 입력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('./count.mp4')\n",
    "width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "# width, height를 선언해서 녹화할 동영상의 프레임 크기를 받아온다.\n",
    "# 프레임 크기는 float 형식으로 반환되므로, int 형식으로 변경함. (변경하지 않을 경우, 동영상 저장 함수에서 오류가 발생함.)\n",
    "\n",
    "videoWriter = cv2.VideoWriter()\n",
    "# cv2.VideoWriter()를 통해 녹화를 위한 메모리 할당.\n",
    "\n",
    "isWrite = False\n",
    "# isWrite는 특정 프레임만 녹화하기 위해 구분하는 bool 형식 변수.\n",
    "\n",
    "\n",
    "while True:\n",
    "# isWrite가 True일 때만 녹화를 시작함.\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if(capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        capture.open('./count.mp4')\n",
    "        \n",
    "    cv2.imshow('VideoFrame', frame)\n",
    "    key = cv2.waitKey(33)\n",
    "    \n",
    "    \n",
    "    if key == 4:  # Alt + D 의미. 해당 키가 입력 되어있을때, 녹화를 시작함.\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID') # cv2.VideoWriter_fourcc(*'XVID')를 활용하여 코덱을 설정.\n",
    "        \n",
    "        videoWriter.open('./Video.avi', fourcc, 30, (width, height), True)\n",
    "        isWrite = True\n",
    "        \n",
    "    elif key == 24:  # Alt + X 의미, 해당 키가 입력되었을때, 녹화를 종료함.\n",
    "        videoWriter.release()  # 메모리 할당을 해제하고 녹화 함수를 종료함.\n",
    "        isWrite = False\n",
    "        \n",
    "    elif key == ord('q'): break\n",
    "        \n",
    "    if isWrite == True:\n",
    "        videoWriter.write(frame)\n",
    "        \n",
    "videoWriter.release()\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 변형\n",
    "## 1. 색상 공간 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본래의 색상 공간에서 다른 생삭 공간으로 변환활 때 사용. 색상 공간 변환 함수는 데이터 타입을 같게 유지하고 채널을 변환함. </p>\n",
    "색상 공간 변환 함수: 입력 이미지(src)에 색상 변환 코드를 적용해서 출력 이미지(dst)로 변환. (색상 변환 코드를 사용해서 BGR 색상 공간을 RGBA 색상 공간으로 변환하거나 그레이스케일, HSV, CIE, Luv 등 단일 채널부터 3채널, 4채널의 색상 공간으로도 변환이 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "dst = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)  # 다중 채널 색상 이미지(HSV)로 변환\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HSV 색상 공간\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "색상을 표현하기에 가장 간편한 색상 공간. 이미지에서 색상을 검출한다고 가정할 때, BGR이나, RGB 패턴으로는 인간이 인지하는 영역의 색상을 구별하기에는 매우 어렵고 복잡하지만, HSV 색상 공간을 활용하면 간편하고 빠르게 특정 색상을 검출하고 분리할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "색상을 검출하려면 각 매개변수에 색상의 범위를 상수 값으로 할당해야함. 그러므로 각 속성이 어떤 역할을 하는지, 어떤 범위를 갖는지 등을 충분히 이해해야함. </p>\n",
    "- 색상(Hue): 세 속성 중 하나로, 색깔의 질을 의미. 빨간, 노랑, 파랑 등의 표현으로 나타내는 성질.\n",
    "- 채도(Saturation): 색의 선명도를 의미하며, 아무것도 섞지 않아 맑고 깨끗하며 원색에 가까운 것을 채도가 높다고 표현.\n",
    "- 명도(Value): 색의 밝기. 명도가 높을수록 백색에, 낮을수록 흑색에 가까워짐.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이미지에서 빨간색 색상을 검출한다면 원형 색상의 범위로 인해 최솟값과 최댓값의 범위를 지정하는 데 어려움을 겪는다. 단순히 최솟값을 약 170, 최댓값을 약 10으로 설정해서 검출을 진행할 경우 오류가 발생한다. 이를 해결하기 위해 두 번에 걸쳐 색상 채널을 나누고 합치는 연산을 해야한다.</p>\n",
    "- 빨간색 계열을 나눠서 낮은 쪽의 빨간색 채널, 높은 쪽의 빨간색 채널을 만든 후 채녈을 합산함. 이를 위해 먼저 색상 공간을 채널별로 나눠야함.\n",
    "\n",
    "\n",
    "### 채널 분리 함수: cv2.split()\n",
    "\n",
    "채널 분리 함수는 다중 입력 이미지(src)를 단일 채널 이미지 배열(mv)로 나눈다. 3채널 이미지를 분리할 경우 단일 채널 이미지로 나눠져 세 개의 결과 이미지로 생성된다. mv 배열 안에는 첫 번째 채널 (mv[0]), 두 번째 채널(mv[1]), 세 번째 채널(mv[2])이 포함돼 있다. Python OpenCV에서는 리스트 형태로 반환됨.</p>\n",
    "\n",
    "### 채널 병합 함수: cv2.merge()\n",
    "\n",
    "채널 병합 함수는 단일 채널 이미지 배열(mv)를 병합해 하나의 출력 이미지(dst)로 반환한다. 채널 분리 함수와 반대로 작동하며, mv 배열 안에는 첫 번째 채널(mv[0]), 두 번째 채널(mv[1]), 세 번째 채널(mv[2])이 포함돼야 한다. 또한 mv 배열의 첫 번째 채널이 채널 별하의 기준이 되어 모든 채널의 속성이 첫 번째 채널의 속성과 일치해야 한다.\n",
    "\n",
    "\n",
    "### 배열 요소의 범위 설정 함수: cv2.InRange()\n",
    "\n",
    "다중 채널 이미지에서 단일 채널을 갖는 이미지들로 분리했다면 해당 채널에서 특정 범위의 값으로 검출해야함. 검출하려는 값과 일치하는 범위는 255를 할당하고 검출하려는 값과 일치하지 않는 범위는 0의 값을 할당한다. 이때 배열 요소의 범위 설정 함수를 사용.</p>\n",
    "\n",
    "배열 요소 범위 설정 함수는 입력 이미지(src)에서 낮은 범위(lowerb)에서 높은 범위(upperb) 사이의 요소를 검출한다. 범위 안에 포함되는 값을 255로 변경하며, 포함되지 않는 값은 0으로 변경해서 출력 이미지(dst)로 반환한다.</p>\n",
    "\n",
    "Python OpenCV에서는 튜플 자료형을 사용해 범위를 할당함. 단일 채널 이미지의 경우 int 형식으로 v값을 할당해서 사용하며, 다중 채널 이미지의 경우 (v0, v1, v2)형식으로 할당한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)  #원본 이미지를 HSV 색상 공간 이미지로 변환.\n",
    "\n",
    "h, s, v = cv2.split(hsv)  # HSV 색상 공간(배열 이미지)에 채널을 분리하여 할당.\n",
    "h_red = cv2.inRange(h,0,5)  # Hue채널에 범위를 할당한 후 빨간색 색상 범위를 갖는 객체만 검출.\n",
    "\n",
    "dst = cv2.bitwise_and(hsv, hsv, mask=h_red)  \n",
    "# h_red 이미지는 Hue 채널에서 0~5 사이의 값을 지니는 요서만 255 값으로 변경하고 나머지는 모두 0으로 변경한다.\n",
    "# 특정 요소가 검출이 완료되면 HSV 이미지 위에 마스크를 씌워 검출된 요소만 보이게 한다.\n",
    "\n",
    "dst = cv2.cvtColor(dst, cv2.COLOR_HSV2BGR)\n",
    "# cv2.imshow()함수는 BGR색상 공간만 정상적으로 출력하므로, HSV 색상 공간을 다시 BGR 색상 공간으로 변경한다.\n",
    "\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비교적 우수하게 검풀에 성공했지만 부정확한 요소를 검출하거나 거의 완벽하게 검출에 성공하지는 못했다. 단순히 HSV 공간 중 Hue의 공간만 활용해 검출했지 때문이다. 도한 붉은색은 앞선 Hue 공간의 범위에서 알 수 잇듯이, 170 이상의 값도 붉은 색에 포함된다. </p>\n",
    "이 문제를 해결하려면 배열 요소의 범위 설정 함수를 HSV 색상 공간으로 범위를 설정하고 검출한 두 요소의 배열을 병합해서 하나의 공간으로 만들어야 한다.</p>\n",
    "\n",
    "\n",
    "### 배열 병합 함수: cv2.addWeighted()\n",
    "\n",
    "배열 병합 함수는 입력 이미지1(src1)에 대한 가중치(alpha) 곱과 입력 이미지2(src2)에 대한 가중치(beta) 곱의 합 중 추가 합(gamma)를 더해서 계산한다. 선택 깊이(dtype)는 정밀도를 임의로 설정할 수 있다. 기본값을 사용할 경우 입력 이미지1(src1)의 정밀도로 설정된다.</p>\n",
    "\n",
    "- 배열 병합 함수 수식: dst = (src1 * alpha) + (src2 * beta) + gamma\n",
    "\n",
    "배열 병합 함수는 알파 블렌딩(alpha blending)을 구현할 수 있어서 서로 다른 이미지를 불투명하게 혼합해서 표시할 수 있다.</p>\n",
    "입력 이미지1과 입력 이미지2를 어떠한 변화 없이 사용할 경우, alpha값은 1.0, beta값은 0.0으로 할당해서 사용한다. 출력 이미지는 두 입력 이미지의 정밀도가 같으므로 기본값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv2.split(hsv)\n",
    "\n",
    "orange = cv2.inRange(hsv, (8, 100, 100), (20, 255, 255))\n",
    "blue = cv2.inRange(hsv, (110, 100, 100), (130, 255, 255))\n",
    "mix_color = cv2.addWeighted(orange, 10., blue, 1.0, 0,0)   \n",
    "# 배열 병합 함수를 활용해 orangr와 blue의 배열을 병합해 mix_color의 배열로 출력한다.\n",
    "# 함수에서 가중치의 할당 없이 병합하므로, alpha는 1.0, beta는 0.0의 값을 사용한다.\n",
    "\n",
    "dst = cv2.bitwise_and(hsv, hsv, mask =mix_color)\n",
    "dst = cv2.cvtColor(dst, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 출력 결과에서 주황색 객체와 파란색 객체 두 가지 색상이 반환된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 이진화\n",
    "동영상이나  이미지에서 어느 지점을 기준으로 픽셀을 분류해서 제외해야 할 때가 있다. 이때 특정 값을 기준으로 값이 높거나 낮은 픽셀을 검은색 또는 흰색의 값으로 변경한다. 즉, 기준값에 따라 이분법적으로 구분해서 픽셀을 참 또는 거짓으로 나누는 연산이며, 이미지 행렬에서 모든 픽셀에 대해 이러한 연산을 수행하는 것이 이진화. </p>\n",
    "\n",
    "\n",
    "### 이진화 함수: cv2.threshold()\n",
    "이진화 함수는 입력 이미지(src)를 임곗값 형식(type)에 따라 특정한 비교 연산을 진행한다. 임곗값(thresh)보다 낮은 픽셀값은 0이나 원본 필셀값으로 변경하며, 임곗값(thresh)보다 높은 픽셀값은 최댓값(maxval)으로 변경한다. 변형된 이미지는 출력 이미지(dst)에 저장되며, Python OpenCV에서는 설정 임곗값(retval)도 반환된다.</p>\n",
    "\n",
    "일반적으로 이진화 함수는 단일 채널 이미지에서 활용되며, 다중 채널 이미지에 이진화 함수를 적용할 경우 각 채널을 분리해서 이진화 함수를 적용한 후 이미지를 다시 병합해서 반환한다. 특정 임곗값 형식에서는 단일 채널 이미지만을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "_, binary = cv2.threshold(src, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('binary', binary)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 이미지에 직접적으로 이진화 함수를 적용한 예제. 색상이 극단적으로 표현되는 것을 알 수 있다. 채널마다 임곗값을 적용해서 반환하므로,  각 채널은 두 종류의 값으로 나뉜다. 결국 다중 채널 이미지에 이진화가 적용된 채널들이 다시 병합되어 하나의 이미지로 변해서 활용하기 어려운 이미지가 된다. 이러한 문제로 특별한 경우가 아닌 이상 다중 채널 이미지에는 이진화 함수를 적용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오츠 알고리즘\n",
    "입력된 이미지의 밝기 분포(히스토그램)를 통해 최적의 임곗값을 찾아 이진화를 적용하는 알고리즘. 가능한 모든 임곗값을 고려해서 이미지 내의 픽셀들을 두 개의 클래스로 분류했을 때 클래스 간의 분산을 최소화하거나 차이를 최대화하는 임곗값을 찾는다.</p>\n",
    "- 오츠 알고리즘 수식: ![image.png](./수식1.png)\n",
    "\n",
    "(수식을 최소화하는 임곗값을 찾는다.)\n",
    "\n",
    "\n",
    "### 삼각형 알고리즘\n",
    "오츠 알고리즘과 동일하게 입력된 이미지의 밝기 분포(히스토그램)를 통해 최적의 임곗값을 찾아 이진화를 적용하는 알고리즘. 오츠 알고리즘과는 다르게 모든 임곗값을 대입하지는 않음. 삼각형 알고리즘은 히스토그램에서 최대 거리를 구성할 수 있는 임곗값을 찾아 이진화를 적용. </p>\n",
    "이때, 최대 거리를 구성할 수 있는 임곗값을 찾아 직각 삼각형으로 만드는 것. 삼각형의 빗변 사이의 거리가 최대일 때 수직인 선이 히스토그램의 최대 거리가 된다. 즉, 히스토그램에 그려진 선 사이의 거리가 최대인 지역값이 임곗값이 됨.</p>\n",
    "\n",
    "\n",
    "#### 적응형 이진화 알고리즘\n",
    "입력 이미지에 따라 입곗값이 스스로 다른 값을 할당할 수 있도록 구성된 이진화 알고리즘. 이미지에 따라 어떠한 임곗값을 주더라도 이진화 처리가 어려운 이미지가 존재함. 이러한 경우 적응형 이진화 알고리즘을 적용.\n",
    "\n",
    "- 적응형 이진화 함수: cv2.adaptiveThreshold()\n",
    "\n",
    "이진화 함수에서 사용하는 최댓값 플래그와 임곗값 형식 플래그를 동일하게 사용. 각 픽셀 주변의 blockSize X blockSize 영역에 대한 가중 평균을 계산. 이후 가중 평균에서 상수 C를 감산한 값을 계산해서 픽셀마다 적응형 임곗값 T(x,y)를 설정. </p>\n",
    "주변 영역 크기인 blockSize와 상수 C에 따라 설정되는 임곗값의 결과가 크게 달라진다. blockSize는 중심점이 존재할 수 있게 홀수만 가능하며 상수 C는 일반적으로 양수의 값을 사용하지만 경우에 따라 0이나 음수도 사용 가능. 또한 적응형 이진화 방식에 따라 결과가 변화며 OpenCV는 두 가지 알고리즘을 지원함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 33, -5)\n",
    "\n",
    "cv2.imshow('binary', binary)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이스케일 이미지에 평균 가중치를 적용한 적응형 이진화. blockSize에 33을 지정하여 33X33 크기 내의 영역을 분석해 적절한 임곗값을 설정한다. 상수 C에는 음수 값인 -5를 지정해 전체 영역이 어두워짐. 음수 값을 지정할 때는 임곗값 형식(thresholdType)에 반전 이진화 플래그(cv.THRESH_BINARY_INV)를 적용하거나 이미지 반전 역산을 적용해 현재 구상하는 알고리즘에 맞는 데이터를 얻어낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지 연산\n",
    "하나 또는 둘 이상의 이미지에 대해 수학적인 연산을 수행하는 것.\n",
    "\n",
    "\n",
    "### 덧셈 함수\n",
    "- cv2.add()\n",
    "\n",
    "배열과 배열 또는 배열과 스칼라의 요소별 합을 계산함. 정밀도에 따라 요소의 최댓값과 최솟값이 있으며, 최댓값을 넘어가거나 최솟값보다 낮아질 수 없다. 덧셈 함수는 최댓값을 넘어가기 쉽기 때문에 이미지 연산 시 두 배열의 요솟값을 고려해서 사용한다.\n",
    "\n",
    "### 뺄셈 함수\n",
    "- cv2.subtract()\n",
    "\n",
    "### 곱셈 함수\n",
    "- cv2.multiply()\n",
    "\n",
    "### 나눗셈 함수\n",
    "- cv2.divide()\n",
    "\n",
    "### 최댓값 함수\n",
    "- cv2.max()\n",
    "\n",
    "\n",
    "### 최솟값 함수\n",
    "- cv2.min()\n",
    "\n",
    "### 절댓값 함수\n",
    "- cv2.abs()\n",
    "\n",
    "### 절댓값 차이 함수\n",
    "- cv2.absdiff()\n",
    "\n",
    "### 비교 함수\n",
    "- cv2.compare()\n",
    "\n",
    "### 선형 방정식 시스템의 해 찾기 함수\n",
    "- cv2.solve()\n",
    "\n",
    "역함수를 기반으로 선형 시스템의 해를 빠르게 구해서 반환함. 부동 소수점 형식만 지원됨.\n",
    "\n",
    "### AND 연산 함수\n",
    "- cv2.bitwise_and()\n",
    "\n",
    "### OR 연산 함수\n",
    "- cv2.bitwise_or()\n",
    "\n",
    "### XOR 연산 함수\n",
    "- cv2.bitwise_xor()\n",
    "\n",
    "### NOT 연산 함수\n",
    "- cv2.bitwise_not()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, array([[4.],\n",
      "       [1.]]))\n"
     ]
    }
   ],
   "source": [
    "src1 = np.array([[9,2], [1,1]], dtype=np.double)\n",
    "src2 = np.array([38,5], dtype=np.double)\n",
    "\n",
    "dst = cv2.solve(src1, src2, flags=cv2.DECOMP_LU)\n",
    "print(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 흐림효과\n",
    "블러링(Blurring) 또는 스무딩(smoothing)이라 불리며, 노이즈를 줄이거나 외부 영향을 최소화하는 데 사용된다. 흐림 효과는 단순히 이미지를 흐리게 만드는 것뿐만 아니라 노이즈를 제거해서 연산 시 계산을 빠르고 정확하게 수행하는 데 도움이 된다. 또한 이미지의 해상도를 변경하는 경우에도 사용되는데 이미지의 크기를 변경하면 존재하지 않는 데이터를 생성하거나 존재하는 데이터를 줄여야 하므로 샘플링된 이미지를 재구성할 때 사용된다.</p>\n",
    "\n",
    "흐림 효과는 영상이나 이미지를 번지게 하며, 해당 픽셀의 주변 값들과 비교하고 게산해서 픽셀들의 색상을 재조정한다. 흐림 효과 함수는 크게 다섯가지의 종류가 있으며, 세 가지 중요 매개변수가 있다. 이 매개변수에 따라 이미지에 흐림 효과를 어떻게 처리할 지가 결정된다.</p>\n",
    "\n",
    "\n",
    "#### 커널과 고정점\n",
    "커널(Kernel)은 이미지에서 (x,y)의 픽셀과 해당 픽셀 주변을 포함한 작은 크기의 공간을 의미하며, 이 영역 각각에 특정한 수식이나 함수 등을 적용해 새로운 이미지를 얻는 알고리즘에서 사용된다. 커널은 영역의 형태와 요소가 결합되는 방식을 정의하는 템플릿을 의미하기도 한다.</p>\n",
    "\n",
    "새로운 픽셀을 만들어 내기 위해 커널 크기의 화소 값을 이용해 어떤 시스템을 통괗 계산하는 것을 컨볼루션(Convolution)이라 한다. 컨벌루션의 예로 이미지를 흐리게 만드는 블러링(Blurring), 이미지의 윤곽을 선명하게 만드는 샤프닝(Sharpening), 이미지 명도의 변화량을 구하는 미분(Gradient, Laplaian) 등이 있다.</p>\n",
    "\n",
    "고정점(anchor point)은 커널을 통해 컨볼루션된 값을 할당한 지점이다. 커널 내에서 고정점은 하나의 지점을 가지며, 이미지와 어떻게 정렬되는지를 나타낸다. </p>\n",
    "\n",
    "\n",
    "#### 테두리 외삽법\n",
    "컨볼루션을 적용할 때 이미지의 가장자리 부분은 계산이 불가능하다. 커널을 활용하는 연산은 모두 이러한 문제에 부딪힌다. 이 문제를 해결하기 위해 텥두리의 이미지 바깥족에 가상의 픽셀을 만들어 처리한다. </p>\n",
    "외삽법으로 가상의 픽셀의 값을 할당하는데, 가상 픽셀의 값을 0으로 처리하거나 커널이 연산할 수 잇는 부분부터 연산을 수행하기도 한다. 또는 이미지의 시작과 끝을 연결해서 폐곡선을 형성해 이미지의 테두리 부분을 대신하게 한다. 커널을 활용하는 함수는 대부분 테두리 외삽법을 설정하는 매개변수를 가지고 있다.</p>\n",
    "\n",
    "\n",
    "### 단순 흐림 효과\n",
    "- cv.blur(src, ksize, anchor=None, borderType=None)\n",
    "\n",
    "입력 이미지(src)의 각 픽셀에 대해 커널을 적용해 모든 픽셀의 단순 평균을 구해 출력 이미지(dst)에 저장한다. 커널의 크기는 ksize를 통해 설정하며, anchor는 커널을 정렬하는 방식을 지정해 고정점을 설정하는 데 사용된다. null값이나 None값을 사용하면 고정점의 위치는 (-1,-1)을 갖게된다. 이 값은 커널을 기준으로 중앙에 위치함을 의미한다. 3X3 크기의 커널일 경우 중심점은 (1,1)이므로 (-1,-1)은 (1,1)의 위치를 갖는다. 다중 채널 이미지의 경우 채널별로 단순 평균값이 계산된다. borderType은 테두리 외삽법을 의미한다.</p>\n",
    "\n",
    "\n",
    "### 박스 필터 흐림 효과\n",
    "- cv2.boxFilter(src, ddepth, ksize, anchor=None, normalize=None, borderType=None)\n",
    "\n",
    "커널의 내부 값이 모두 값은 필터. ddepth는 출력 이미지의 정밀도를 설정한다. ddepth값을 cv2.CV_8U 형식으로 입력하며, -1로 설정한다면 입력 이미지와 동일한 정밀도가 설정된다. normalize는 커널이 영역별 정규화 여부를 설정ㅎㄴ다. 박스 필터 함수는 일반적으로 커널의 모든 값이 1의 값을 갖는다. 하지만 normalize의 값을 True로 지정할 경우 정규화된 박스 필터로 변경되며, 커널의 모든 값이 커널의 개수만큼 나눠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 변환\n",
    "이미지 변형과 동일하게 전처리 과정으로 주로 활용된다. 이미지 변형은 기본적으로 데이터의 개수가 감소하지는 않지만 이미지 해석을 위해 조금 더 검출하기 쉬운 데이터로 만드는 과정이다. </p>\n",
    "이미지 변환은 데이터의 개수를 늘리거나 줄여서 알고리즘의 연산량을 줄이는 것을 주목적으로 삼는다.</p>\n",
    "\n",
    "1. 강체 변환: 평행 이동, 회전\n",
    "2. 유사 변환: 평행 이동, 회전, 크기 변환\n",
    "3. 선형 변환: 크기 변환, 반사, 기울임\n",
    "4. 아핀 변환: 크기 변환, 반사, 기울임, 이동 변환(선의 수평성을 유지)\n",
    "5. 원근 변환: 아핀 변환과 비슷하지만 아핀 변환에서 유지되는 수평성은 유지되지 않음.\n",
    "\n",
    "\n",
    "## 1 확대 & 축소\n",
    "이미지 피라미드를 활용해서 이미지의 크기를 원하는 단계까지 샘플링하는 작업.\n",
    "\n",
    "\n",
    "### 이미지 확대\n",
    "- cv.pyrUp(src, dstSize=None, borderType=None)\n",
    "\n",
    "입력 이미지(src)의 행과 열을 2배로 키워 이미지를 확대하는 변환 함수이다. 이미지를 업샘플링한 후 흐림 효과를 적용한다. 출력 이미지 크기(dstSize)는 출력 이미지(dst)의 크기를 나타낸다. 이미지 확대 함수는 입력 이미지에 새로운 행과 열을 추가해서 짝수 행과 짝수 열을 만들고 0의 값을 할당한다. 이후 새로운 값을 할당해야 하므로 색상이 결정되지 않는 픽셀은 근삿값으로 채워 가우시안 필터로 컨볼루션을 진행한다. 가우시안 필터는 4의 값을 사용해 정규화한다.</p>\n",
    "\n",
    "\n",
    "### 이미지 축소\n",
    "- cv2.pyrDown(src, dstSize=None, borderType=None)\n",
    "\n",
    "입력 이미지(src)의 행과 열을 2배로 축소해서 이미지를 축소하는 변환 함수다. 이미지에 흐림 효과를 적용한 후 다운샘플링을 적용한다. 이미지를 2배 작게 만들어야 하므로 홀수 크기의 이미지에는 +1을 해서 짝수 크기의 이미지로 변경한다. Gi 레이어를 컨볼루션하고 모든 짝수 행과 짝수 열을 제거해 Gi+1을 생성한다. 생성된 레이어는 입력 이미지(src)의 1/4에 해당하는 면적을 갖는다. 출력 이미지(dst)의 크기는 2배 축소한 이미지가 된다. </p>\n",
    "\n",
    "\n",
    "### 이미지 크기 조절\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "dst = src.copy()\n",
    "\n",
    "for i in range(3):\n",
    "    dst = cv2.pyrDown(dst)\n",
    "    \n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반복문을 활용해 다운샘플링을 3회 적용해서 출력 이미지(dst)의 크기를 1/8배 크기로 변환. 입력 이미지(src)와 출력 이미지(dst) 매개변수에 동일한 변수를 구성한다면 간단하게 다운샘플링이 연속적으로 적용되도록 구성할 수 있다.\n",
    "\n",
    "\n",
    "## 2 이미지 크기 조절\n",
    "사용자가 원하는 크기로 이미지를 변환하기.</p>\n",
    "1. 이미지의 크기를 사용자가 요구하는 절대 크기로 변경.\n",
    "2. 이미지의 크기를 비율에 맞게 상대적인 크기로 변경 (입력 이미지의 크기와 비례하도록 너비와 높이가 계산됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv2.reSize(src, dsize, fx=None, fy=None, interpolation=None)\n",
    "\n",
    "입력 이미지(src)를 절대 크기(dsize)나 상대 크기(fx,fy)로 변환. 절대 크기(dsize)는 필수 매개변수, 상대 크기로 변환하기 위해서는 절대 크기의 값을 (0,0)으로 설정하고 fx와 fy에 각각 x축과 y축에 적용할 비율 계수를 설정한다. interpolation은 이미지의 크기를 조절할 때 사용할 보간법을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "\n",
    "dst = src[280:310, 240:405]\n",
    "dst = cv2.resize(dst, dsize=(256,256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 대칭 & 회전\n",
    "대칭은 기하학적인 측면에서 반사(reflection)의 의미를 갖는다. 2차원 유클리드 공간에서의 기하학적인 변환의 하나로 R^2(2차원 유클리드 공간) 위의 선형 변환을 진행한다. 대칭은 변환할 행렬에 대해 2X2 행렬을 왼쪽 곱셈한다. 즉 'p'형태의 물체에 Y축 대칭을 적용한다면 'q'형태를 띈다.\n",
    "- cv2.flip(src, flipCode)\n",
    "\n",
    "입력 이미지(src)의 행과 열을 바꾸기 위해 축을 기준으로 이미지를 반사하는 함수. X축, Y축 XY축을 대상으로 이미지를 대칭시키고, 대칭 축(flipCode)을 사용해 이미지의 대칭 방향을 선택한다. 대칭축의 값이 음수일 경우 XY축 대칭을 수행하며, 0이라면 X축을 기준으로 대칭을 진행한다. 양수일 경우 Y축을 기준으로 대칭한다. 대칭 함수의 플래그는 양수, 0, 음수 값으로 표기하므로 다른 함수와 다르게 OR(|) 연산이 가능하지 않다.</p>\n",
    "\n",
    "\n",
    "회전 또한 선형 변환 중 하나에 포함되며, 회전 변환 행렬은 임의의 점을 중심으로 물체를 회전시킨다. 회전 변환 행렬의 일부는 반사 행렬과 같은 값을 지닐 수 있다. 2차원 유클리드 공간에서의 회전은 크게 두 가지 회전 행렬을 갖는다. 좌푯값을 회전시키는 회전 행렬과 좌표축을 회전시키는 회전 행렬이 있다.</p>\n",
    "1. 죄표 회전 행렬: 원점을 중심으로 좌푯값을 회전시켜 매핑.\n",
    "2. 좌표축 회전 행렬: 원점을 중심으로 행렬 자체를 회전시켜 새로운 행렬의 값을 구성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(두 회전 모두 원점을 중심으로 계산함.)\n",
    "\n",
    "단순한 회전은 2X2 행렬을 활용해 원하는 결과를 쉽게 얻을 수 있다. 하지만 회전 행렬은 원점 중심으로 한 회전만 가능하다. 임의의 중심점을 기반으로 회전을 수행하기 위해서는 아핀 변환에 기반을 둔 회전 행렬을 활용해야 한다. 이 회전 행렬은 2X2 행렬이 아닌, 2X3 아핀 변환 행렬을 사용해 임의의 중심점을 기준으로 회전할 수 있다.</p>\n",
    "\n",
    "2X3 회전 행렬의 형태를 보면 좌표축의 회전 이동 행렬과 동일한 형태임을 알 수 있다. 좌표축에 대한 회전 행렬은 비율의 조정 없이 중심점을 기준으로 회전한다. 하지만 2X3 회전 행렬을 사용할 경우 회전 축의 기준점 변경과 비율을 조정할 수 있다.</p>\n",
    "- cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "center는 중심점의 좌표, angle은 이미지가 회전될 회전각(반시계 방향), scale은 회전 후의 이미지의 확대 또는 축소 비율이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg')\n",
    "\n",
    "height, width, _ =src.shape\n",
    "center = (width/2, height/2)\n",
    "angle = 30\n",
    "scale = 0.5\n",
    "matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "radians = math.radians(angle)\n",
    "sin = math.sin(radians)\n",
    "cos = math.cos(radians)\n",
    "bound_w = int((height * scale * abs(sin)) + (width * scale * abs(cos)))\n",
    "bound_h = int((height * scale * abs(cos)) + (width * scale * abs(sin)))\n",
    "\n",
    "matrix[0,2] += ((bound_w/2) - center[0])\n",
    "matrix[1,2] += ((bound_h/2) - center[1])\n",
    "\n",
    "dst = cv2.warpAffine(src, matrix, (bound_w, bound_h))\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 기하학적 변환\n",
    "\n",
    "### 아핀 변환\n",
    "\n",
    "### 원근 변환\n",
    "\n",
    "\n",
    "- 넘어감 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 모폴로지 변환\n",
    "영상이나 이미지를 형태학적 관점에서 접근하는 기법을 의미. 모폴로지 변환은 주로 영상 내 픽셀값 대체에 사용됨. 이를 응용해서 노이즈 제거, 오쇼 결합 및 분리, 강도 피크 검출 등에 이용할 수 있다. 이러한 모폴로지 변환을 수행할 때는 집합의 포함 관계, 이동, 대칭, 여집합, 차집합 등의 성질을 이용한다. </p>\n",
    "\n",
    "기본적인 모폴로지 변환으로는 팽창과 침식이 있다. 팽창과 침식은 이미지와 커널의 컨벌루션 연산이며, 이 두 가지 연산을 기본으로 다양한 모폴로지 연산을 구현할 수 있다. 모폴로지 변환은 전처리 또는 후처리 과정에서 가장 많이 사용되는 변환이다. 모폴로지 변환을 통해 피크를 검출하거나 그레디언트를 정의할 수 있다.</p>\n",
    "\n",
    "팽창은 커널 영역 안에 존재하는 모든 픽셀의 값을 커널 내부의 극댓값으로 대체한다. 즉, 구조 요소를 활용해서 이웃한 픽셀을 최대 픽셀값으로 대체한다. 팽창 연산을 적용하면 어두운 영역이 줄어들고 밝은 영역이 늘어난다. 커널의 크기나 반복 횟수에 다라 밝은 영역이 늘어나 스펙클이 커지며, 객체 내부의 홀이 사라진다. 노이즈 제거 후 줄어든 크기를 복구 하고자 할 때 주로 사용한다.</p>\n",
    "\n",
    "\n",
    "침식은 커널 영역 안에 존재하는 모든 픽셀의 값을 커널 내부의 극솟값으로 대체한다. 측, 구조 요소를 활용해 이웃한 픽셀을 최소 픽셀값으로 대체한다. 침식 연산을 적용하면 밝은 영역이 줄어들고 어두운 영역이 늘어난다.커널의 크기나 반복 횟수에 따라 어두운 영역이 늘어나 스펰클이 사라지며, 객체 내부의 홀이 커진다. 노이즈 제거에 주로 사용. </p>\n",
    "\n",
    "모폴로지 연산은 커널의 영향을 크게 받으며 커널의 형태에 따라 결과가 달라진다. 또한 커널보다 더 복잡한 개념인 구조 요소를 사용하며, 커널의 형태를 설정할 수 있다. 커널은 nXn 크기의 직사각형이나 정사각형 구조로만 활용했지만 구조 요소는 직사각형을 비롯해 타원, 십자 모양의 형태로도 활용 가능하다. 모폴로지 변환은 이 구조 요소르 사용해 수행한다. </p>\n",
    "\n",
    "- 구조 요소 생성 함수: cv2.getStructuringElement(shape, ksize,anchor=None)\n",
    "\n",
    "커널의 형태(shape)를 설정할 수 있으며, 직사각형(Rect), 십자가(Cross), 타원(Ellipse) 모양으로 구조 요소를 생성한다. 커널의 크기는 ksize로 설정하며, anchor는 고정점의 위치를 나타낸다. 커널의 크기가 너무 작다면 커널의 형태는 영향을 받지 않는다. 또한, 고정점(anchor)은 필수 매개변수가 아니다.</p>\n",
    "\n",
    "- 팽창 함수: cv2.dilate(src, kernel, anchor=None, iterations=None, borserType=None, borderValue=None)\n",
    "- 침식 함수: cv2.erode(src, kernel, anchor=None, iterations=None, borderType=None, borderValue=None)\n",
    "\n",
    "모폴로지 변환의 팽창과 침식 함수는 동일한 매개변수의 형태를 사용한다. 입력 이미지(src)에 구조 요소(kerenl)를 사용해 팽창 또는 침식을 적용한다. 고정점(anchor)을 함수 내에서 할당할 수 있으며, 반복 횟수(iterations)를 설정해 침식 함수가 몇 회 연산할지 선택한다. 또한 커널을 활용하므로 컨벌루션 연산처럼 테두리 외삽법과 테두리 색상을 설정할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread('./image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5), anchor=(-1, -1))\n",
    "dst = cv2.erode(src, kernel, iterations=3)\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 모폴로지 연산\n",
    "그레이스케일이나 다중 채널 이미지를 사용하는 경우 더 복잡한 연산이 필요하다. 이때 모폴로지 연산을 활용해 우수한 결과를 얻을 수 있다.</p>\n",
    "\n",
    "- 모폴로지의 연산 함수: cv2.orphologyEx(src, op, kernel, anchor=None, iterations=None, borderType=None, borderValue=None)\n",
    "\n",
    "모폴로지 변환에 기반을 두고 있기 때문에 커널(kernel), 반복 횟수(iterations), 테두리 외삽법(borderMpde), 테두리 색상(borderValue)을 동일하게 사용하고, 연산자(op)라는 매개변수를 추가로 받는다. 연산자(op)는 모폴로지 변환 함수를 조합해서 수행하는 복합 연산 방식을 의미하며, 모폴로지 연산은 이러한 연산자 플래그를 변경해 다양한 방식으로 사용할 수 있다.\n",
    "\n",
    "\n",
    "### 열림 연산\n",
    "팽창 연산자와 침식 연산자의 조합. 침식 연산을 적용한 다음, 팽창 연산을 적용한다. 이로 인해 스펙클이 사라지면서 발생한 객체의 크기 감소를 원래대로 복구할 수 있다.\n",
    "\n",
    "### 닫힘 연산\n",
    "팽창 연산자와 침식 연산자의 조합. 열림과 반대로 팽창 연산을 적용한 다음, 침식 연산을 적용. 그로 인해 객체 내부의 홀이 사라지면서 발생한 크기 증가를 원래대로 복구할 수 있다.\n",
    "\n",
    "### 그레디언트 연산\n",
    "팽창 연산자과 침식 연산자의 조합. 열림 연산이나 닫힘 연산과 달리 입력 이미지에 각각 팽창 연산과 침식 연산을 적용하고 감산을 진행. 그레디언트는 밝은 영역의 가장자리를 분리하며 그레이스케일 이미지가 가장 급격하게 변하는 곳에서 가장 높은 결과를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
